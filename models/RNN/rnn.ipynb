{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "name": "rnn.ipynb",
   "provenance": [],
   "machine_shape": "hm"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# RNN Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Install Packages"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Install your required packages here\n",
    "!pip install pandas numpy matplotlib sklearn fsspec gcsfs tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports & Constants"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Import the libraries for RNN LSTM\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from tqdm import tqdm\n",
    "\n",
    "from google.cloud import storage\n",
    "from glob import glob\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.utils.vis_utils import plot_model\n",
    "import math\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Commented out IPython magic to ensure Python compatibility.\n",
    "%env GOOGLE_APPLICATION_CREDENTIALS=/content/drive/My Drive/CS/AI/Credentials/ai-project-2020-f4dfbc25326c.json"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bucket_name = \"ai-project-2020-spotify\"\n",
    "client = storage.Client()\n",
    "bucket = client.get_bucket(bucket_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import Session Logs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "logs = pd.read_csv(f\"gs://{bucket_name}/training_set/log_0_20180715_000000000000.csv.gz\")\n",
    "\n",
    "logs.drop(columns=['date'], inplace=True)\n",
    "\n",
    "track_num_columns_old = ['duration', 'release_year', 'us_popularity_estimate',\n",
    "                     'acousticness', 'beat_strength', 'bounciness', 'danceability',\n",
    "                     'dyn_range_mean', 'energy', 'flatness', 'instrumentalness', 'key',\n",
    "                     'liveness', 'loudness', 'mechanism', 'organism', 'speechiness',\n",
    "                     'tempo', 'time_signature', 'valence', 'acoustic_vector_0',\n",
    "                     'acoustic_vector_1', 'acoustic_vector_2', 'acoustic_vector_3',\n",
    "                     'acoustic_vector_4', 'acoustic_vector_5', 'acoustic_vector_6',\n",
    "                     'acoustic_vector_7']\n",
    "track_num_columns = track_num_columns_old + [\"context_switch\", \"no_pause_before_play\",\n",
    "                                             \"short_pause_before_play\", \"long_pause_before_play\",\n",
    "                                             'hist_user_behavior_n_seekfwd', 'hist_user_behavior_n_seekback',\n",
    "                                            'hist_user_behavior_is_shuffle', \"premium\"]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import Track Features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "track_features_1 = pd.read_csv('/content/drive/My Drive/CS/AI/Data/track_features/tf_000000000000.csv').set_index('track_id')\n",
    "track_features_2 = pd.read_csv('/content/drive/My Drive/CS/AI/Data/track_features/tf_000000000001.csv').set_index('track_id')\n",
    "track_features = track_features_1.append(track_features_2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "track_features.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Utility Functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def ave_pre(submission, groundtruth):\n",
    "    \"\"\" Calculate average accuracy (which is the same as average precision in this context) \"\"\"\n",
    "    s = 0.0\n",
    "    t = 0.0\n",
    "    c = 1.0\n",
    "    for x, y in zip(submission, groundtruth):\n",
    "        if x != 0 and x != 1:\n",
    "            raise ValueError()\n",
    "        if x == y:\n",
    "            s += 1.0\n",
    "            t += s / c\n",
    "        c += 1\n",
    "    return t / len(groundtruth)\n",
    "\n",
    "def evaluate(submission, groundtruth):\n",
    "    \"\"\" Calculate metrics for prediction and ground thruth lists (source: starter kit) \"\"\"\n",
    "    ap_sum = 0.0\n",
    "    first_pred_acc_sum = 0.0\n",
    "    counter = 0\n",
    "    for sub, tru in zip(submission, groundtruth):\n",
    "        if len(sub) != len(tru):\n",
    "            raise Exception('Line {} should contain {} predictions, but instead contains '\n",
    "                            '{}'.format(counter + 1, len(tru), len(sub)))\n",
    "        try:\n",
    "            ap_sum += ave_pre(sub, tru)\n",
    "        except ValueError as e:\n",
    "            raise ValueError('Invalid prediction in line {}, should be 0 or 1'.format(counter))\n",
    "        first_pred_acc_sum += sub[0] == tru[0]\n",
    "        counter += 1\n",
    "    ap = ap_sum / counter\n",
    "    first_pred_acc = first_pred_acc_sum / counter\n",
    "    return ap, first_pred_acc\n",
    "\n",
    "def normalize(df,feature_name):\n",
    "    result = df.copy()\n",
    "    for name in feature_name:\n",
    "        max_value = df[name].max()\n",
    "        min_value = df[name].min()\n",
    "        result[name] = (df[name] - min_value) / (max_value - min_value)\n",
    "    return result\n",
    "\n",
    "def categorical_to_dummies(df, categorical_cols):\n",
    "    \"\"\" Create dummies (one hot encoding) for each categorical variables \"\"\"\n",
    "    dummies = pd.get_dummies(df[categorical_cols], prefix=categorical_cols)\n",
    "    return df.drop(columns=categorical_cols).join(dummies)\n",
    "\n",
    "def split_sessions(data, perc_in=0.6):\n",
    "    \"\"\" Split interactions into train and test sessions. \"\"\"\n",
    "    sessions = data['session_id'].unique()\n",
    "    amt_in = int(perc_in * len(sessions))\n",
    "    sessions_in = np.random.choice(sessions, amt_in, replace=False)\n",
    "    sessions_out = np.array(list(set(sessions) - set(sessions_in)))\n",
    "    indexed_data = data.set_index('session_id')\n",
    "    data_in = indexed_data.loc[sessions_in]\n",
    "    data_out = indexed_data.loc[sessions_out]\n",
    "    return data_in, data_out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "logs = categorical_to_dummies(logs, ['context_type', 'hist_user_behavior_reason_start', 'hist_user_behavior_reason_end'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "track_num_columns += list(logs.columns[18:])\n",
    "\n",
    "track_features = categorical_to_dummies(track_features, ['mode'])\n",
    "\n",
    "track_num_columns += ['mode_major', 'mode_minor']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Join track features and logs\n",
    "data = logs.join(track_features, on='track_id_clean', how='left')\n",
    "data['session_id'].nunique()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Convert booleans to ints\n",
    "data['premium'] = data['premium']*1\n",
    "data['hist_user_behavior_is_shuffle'] = data['hist_user_behavior_is_shuffle']*1\n",
    "data['skip_1'] = data['skip_1']*1\n",
    "data['skip_2'] = data['skip_2']*1\n",
    "data['skip_3'] = data['skip_3']*1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Normalize\n",
    "feature_name = ['duration',\n",
    " 'release_year',\n",
    " 'us_popularity_estimate',\n",
    " 'flatness',\n",
    " 'loudness',\n",
    " 'tempo',\n",
    " 'acoustic_vector_0',\n",
    " 'acoustic_vector_1',\n",
    " 'acoustic_vector_2',\n",
    " 'acoustic_vector_3',\n",
    " 'acoustic_vector_4',\n",
    " 'acoustic_vector_5',\n",
    " 'acoustic_vector_6',\n",
    " 'acoustic_vector_7',\n",
    " 'key']\n",
    "\n",
    "data = normalize(data, feature_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "train, val = split_sessions(data, 0.75)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_matrix(data):\n",
    "    data1 = data[data.session_position <= (data.session_length / 2)]\n",
    "    data2 = data[data.session_position > (data.session_length / 2)]\n",
    "    \n",
    "    # Split into first and second part\n",
    "    start_sessions = data1.groupby(\"session_id\")\n",
    "    end_sessions = data2.groupby(\"session_id\")\n",
    "    \n",
    "    # Arrays\n",
    "    X1, X2, y = [], [], []\n",
    "    \n",
    "    # For first part\n",
    "    for session_id in tqdm(start_sessions.groups.keys()):\n",
    "        # Get columns based on group\n",
    "        session = start_sessions.get_group(session_id)[track_num_columns + [\"skip_2\"]]\n",
    "        # Set skip_2\n",
    "        session[\"skip_2\"] = session[\"skip_2\"] * 1\n",
    "        # print(session.info())\n",
    "        x = session.to_numpy()\n",
    "        # print(x)\n",
    "        # Padding operation\n",
    "        X1.append(np.pad(x, ((0, 10 - len(x)), (0, 0)), 'constant', constant_values=(0)))\n",
    "        # print(X1)\n",
    "    \n",
    "    # For second part\n",
    "    for session_id in tqdm(end_sessions.groups.keys()):\n",
    "        session = end_sessions.get_group(session_id)[track_num_columns_old + [\"skip_2\"]]\n",
    "        # Set \n",
    "        true_y = session[\"skip_2\"].to_numpy() * 1\n",
    "        # print(session)\n",
    "        # remove skip_2 from second part\n",
    "        del session[\"skip_2\"]\n",
    "        x = session.to_numpy()\n",
    "        X2.append(np.pad(x, ((0, 10 - len(x)), (0, 0)), 'constant', constant_values=(0)))\n",
    "        y.append(np.pad(true_y, (0, 10 - len(true_y)), 'constant', constant_values=(0)))\n",
    "    \n",
    "    X1 = np.array(X1)\n",
    "    X2 = np.array(X2)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    return X1,X2,y\n",
    "\n",
    "X1,X2,y = create_matrix(train)\n",
    "\n",
    "X1_v,X2_v,y_v = create_matrix(val)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# \"\"\"## Loading numpy arrays (from archives)\n",
    "# This is alternative to loading from Google Drive.\n",
    "# Can be used to speed up experimentation once archives have been created.\n",
    "# \"\"\"\n",
    "\n",
    "# train_files = [] # Archive files to use for training\n",
    "# test_files = []  # Archive files to use for testing\n",
    "\n",
    "# # Load training files\n",
    "# X1 = None\n",
    "# X2 = None\n",
    "# y = None\n",
    "# for fname in train_files:\n",
    "#   archive = np.load(fname)\n",
    "#   if X1 is None:\n",
    "#     # This is the first file, initialise the arrays\n",
    "#     X1 = archive['X1']\n",
    "#     X2 = archive['X2']\n",
    "#     y = archive['y']\n",
    "#   else:\n",
    "#     # Concatenate the arrays\n",
    "#     X1 = np.concatenate(X1, archive['X1'])\n",
    "#     X2 = np.concatenate(X1, archive['X2'])\n",
    "#     y = np.concatenate(X1, archive['y'])\n",
    "\n",
    "# X1_v = None\n",
    "# X2_v = None\n",
    "# y_v = None\n",
    "# for fname in test_files:\n",
    "#   archive = np.load(fname)\n",
    "#   if X1 is None:\n",
    "#     # This is the first file, initialise the arrays\n",
    "#     X1 = archive['X1']\n",
    "#     X2 = archive['X2']\n",
    "#     y = archive['y']\n",
    "#   else:\n",
    "#     # Concatenate the arrays\n",
    "#     X1 = np.concatenate(X1, archive['X1'])\n",
    "#     X2 = np.concatenate(X1, archive['X2'])\n",
    "#     y = np.concatenate(X1, archive['y'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Network 1 - First half of the session\n",
    "input_layer1 = layers.Input(shape=(10, X1.shape[2]))\n",
    "\n",
    "# Normalisation\n",
    "norm1 = layers.BatchNormalization()(input_layer1)\n",
    "\n",
    "# Recurrent layer(s)\n",
    "lstm1 = layers.Bidirectional(layers.GRU(25, return_sequences=False, input_shape=(10, X1.shape[2])))(norm1)\n",
    "\n",
    "# Network 2 - Second half of the session\n",
    "input_layer2 = layers.Input(shape=(10, X2.shape[2]))\n",
    "\n",
    "# Normalisation\n",
    "norm2 = layers.BatchNormalization()(input_layer2)\n",
    "\n",
    "# Recurrent layer(s)\n",
    "lstm2 = layers.Bidirectional(layers.GRU(25, return_sequences=False, input_shape=(10, X2.shape[2])))(norm2)\n",
    "\n",
    "# Concatenation & dense layer\n",
    "concat = layers.Concatenate()([lstm1, lstm2])\n",
    "dense_last = layers.Dense(10, activation=\"relu\")(concat)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compile\n",
    "lossf = keras.losses.MeanAbsoluteError()\n",
    "model = keras.Model(inputs=[input_layer1, input_layer2], outputs=[dense_last])\n",
    "model.compile(optimizer='adam', loss=lossf, metrics=[\"acc\"])\n",
    "\n",
    "history = model.fit([X1, X2], y, epochs=15, batch_size=100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Predict\n",
    "predictions = model.predict([X1_v, X2_v])\n",
    "\n",
    "evaluate([[i >= 0.5 for i in p] for p in predictions], y_v)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}