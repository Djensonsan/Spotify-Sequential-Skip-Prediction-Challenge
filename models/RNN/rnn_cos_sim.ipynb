{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vr6OFn5V6E3g"
   },
   "source": [
    "# RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FPm1aq_A6E3h"
   },
   "source": [
    "## Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jZMXPTNd6E3h",
    "outputId": "2abfc60f-dc12-4ddc-c222-20376017dad1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (1.1.4)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (1.18.5)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (3.3.2)\n",
      "Requirement already satisfied: sklearn in /opt/conda/lib/python3.7/site-packages (0.0)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (0.8.4)\n",
      "Requirement already satisfied: gcsfs in /opt/conda/lib/python3.7/site-packages (0.7.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (4.51.0)\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.3.1-cp37-cp37m-manylinux2010_x86_64.whl (320.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 320.4 MB 17 kB/s /s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (8.0.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (2020.6.20)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from sklearn) (0.23.2)\n",
      "Requirement already satisfied: google-auth>=1.2 in /opt/conda/lib/python3.7/site-packages (from gcsfs) (1.23.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from gcsfs) (3.7.2)\n",
      "Requirement already satisfied: google-auth-oauthlib in /opt/conda/lib/python3.7/site-packages (from gcsfs) (0.4.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from gcsfs) (2.24.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from gcsfs) (4.4.2)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.33.2)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.11.2)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.13.0)\n",
      "Collecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting keras-preprocessing<1.2,>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 774 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting google-pasta>=0.1.8\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 5.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.4.0,>=2.3.0\n",
      "  Downloading tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n",
      "\u001b[K     |████████████████████████████████| 459 kB 51.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.10.0)\n",
      "Collecting astunparse==1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.35.1)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 4.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py>=0.7.0\n",
      "  Downloading absl_py-0.11.0-py3-none-any.whl (127 kB)\n",
      "\u001b[K     |████████████████████████████████| 127 kB 59.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting tensorboard<3,>=2.3.0\n",
      "  Downloading tensorboard-2.4.0-py3-none-any.whl (10.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.6 MB 68.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.5.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sklearn) (0.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.2->gcsfs) (49.6.0.post20201009)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.2->gcsfs) (0.2.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.2->gcsfs) (4.6)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.2->gcsfs) (4.1.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->gcsfs) (3.7.4.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->gcsfs) (20.2.0)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->gcsfs) (3.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->gcsfs) (1.6.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->gcsfs) (4.7.5)\n",
      "Requirement already satisfied: chardet<4.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->gcsfs) (3.0.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib->gcsfs) (1.3.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->gcsfs) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->gcsfs) (2.10)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\n",
      "\u001b[K     |████████████████████████████████| 779 kB 43.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.3.3)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.0.1)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (3.4.0)\n",
      "Building wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=ecae5c9414eab9fde982c3ec2774297c475008e15085b16d6dd6bdbcdbc05877\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Successfully built termcolor\n",
      "Installing collected packages: gast, keras-preprocessing, google-pasta, tensorflow-estimator, astunparse, opt-einsum, absl-py, termcolor, tensorboard-plugin-wit, tensorboard, tensorflow\n",
      "Successfully installed absl-py-0.11.0 astunparse-1.6.3 gast-0.3.3 google-pasta-0.2.0 keras-preprocessing-1.1.2 opt-einsum-3.3.0 tensorboard-2.4.0 tensorboard-plugin-wit-1.7.0 tensorflow-2.3.1 tensorflow-estimator-2.3.0 termcolor-1.1.0\n"
     ]
    }
   ],
   "source": [
    "# Install your required packages here\n",
    "!pip install pandas numpy matplotlib sklearn fsspec gcsfs tqdm tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2-1zdo96E3h"
   },
   "source": [
    "## Imports & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m22qIuta6E3i",
    "outputId": "ee509f71-0cfe-427f-946c-7f9abe1e7a34"
   },
   "outputs": [],
   "source": [
    "#Import the libraries for RNN LSTM\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.python.keras.utils.vis_utils import plot_model\n",
    "\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y16l75066E3i",
    "outputId": "c3a11c72-031e-4934-e5bb-a20023f12e48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: GOOGLE_APPLICATION_CREDENTIALS=ai-project-2020-f4dfbc25326c.json\n"
     ]
    }
   ],
   "source": [
    "# Commented out IPython magic to ensure Python compatibility.\n",
    "%env GOOGLE_APPLICATION_CREDENTIALS=ai-project-2020-f4dfbc25326c.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "WcxhXxfX6E3i"
   },
   "outputs": [],
   "source": [
    "bucket_name = \"ai-project-2020-spotify\"\n",
    "client = storage.Client()\n",
    "bucket = client.get_bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQd8k-GQ6E3j"
   },
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "pFf__p346E3j"
   },
   "outputs": [],
   "source": [
    "def ave_pre(submission, groundtruth):\n",
    "    \"\"\" Calculate average accuracy (which is the same as average precision in this context) \"\"\"\n",
    "    s = 0.0\n",
    "    t = 0.0\n",
    "    c = 1.0\n",
    "    for x, y in zip(submission, groundtruth):\n",
    "        if x != 0 and x != 1:\n",
    "            raise ValueError()\n",
    "        if x == y:\n",
    "            s += 1.0\n",
    "            t += s / c\n",
    "        c += 1\n",
    "    return t / len(groundtruth)\n",
    "\n",
    "def evaluate(submission, groundtruth):\n",
    "    \"\"\" Calculate metrics for prediction and ground thruth lists (source: starter kit) \"\"\"\n",
    "    ap_sum = 0.0\n",
    "    first_pred_acc_sum = 0.0\n",
    "    counter = 0\n",
    "    for sub, tru in zip(submission, groundtruth):\n",
    "        # if len(sub) != len(tru):\n",
    "        #     raise Exception('Line {} should contain {} predictions, but instead contains '\n",
    "        #                     '{}'.format(counter + 1, len(tru), len(sub)))\n",
    "        try:\n",
    "            ap_sum += ave_pre(sub, tru)\n",
    "        except ValueError as e:\n",
    "            raise ValueError('Invalid prediction in line {}, should be 0 or 1'.format(counter))\n",
    "        first_pred_acc_sum += sub[0] == tru[0]\n",
    "        counter += 1\n",
    "    ap = ap_sum / counter\n",
    "    first_pred_acc = first_pred_acc_sum / counter\n",
    "    return ap, first_pred_acc\n",
    "\n",
    "def normalize(df,feature_name):\n",
    "    result = df.copy()\n",
    "    for name in feature_name:\n",
    "        max_value = df[name].max()\n",
    "        min_value = df[name].min()\n",
    "        result[name] = (df[name] - min_value) / (max_value - min_value)\n",
    "    return result\n",
    "\n",
    "def categorical_to_dummies(df, categorical_cols):\n",
    "    \"\"\" Create dummies (one hot encoding) for each categorical variables \"\"\"\n",
    "    dummies = pd.get_dummies(df[categorical_cols], prefix=categorical_cols)\n",
    "    return df.drop(columns=categorical_cols).join(dummies)\n",
    "\n",
    "def split_sessions(data, perc_in=0.6):\n",
    "    \"\"\" Split interactions into train and test sessions. \"\"\"\n",
    "    sessions = data['session_id'].unique()\n",
    "    amt_in = int(perc_in * len(sessions))\n",
    "    sessions_in = np.random.choice(sessions, amt_in, replace=False)\n",
    "    sessions_out = np.array(list(set(sessions) - set(sessions_in)))\n",
    "    indexed_data = data.set_index('session_id')\n",
    "    data_in = indexed_data.loc[sessions_in]\n",
    "    data_out = indexed_data.loc[sessions_out]\n",
    "    return data_in, data_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "baVXPRTj6E3i"
   },
   "source": [
    "## Import Session Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cxPhc30yDRW1",
    "outputId": "96cd41a2-4b90-42aa-9fa7-3d27bed68377"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_set/log_0_20180715_000000000000.csv.gz\n",
      "training_set/log_1_20180715_000000000000.csv.gz\n",
      "training_set/log_2_20180715_000000000000.csv.gz\n",
      "training_set/log_3_20180715_000000000000.csv.gz\n",
      "training_set/log_4_20180715_000000000000.csv.gz\n",
      "training_set/log_5_20180715_000000000000.csv.gz\n",
      "training_set/log_6_20180715_000000000000.csv.gz\n",
      "training_set/log_7_20180715_000000000000.csv.gz\n",
      "['gs://ai-project-2020-spotify/training_set/log_0_20180715_000000000000.csv.gz', 'gs://ai-project-2020-spotify/training_set/log_1_20180715_000000000000.csv.gz', 'gs://ai-project-2020-spotify/training_set/log_2_20180715_000000000000.csv.gz', 'gs://ai-project-2020-spotify/training_set/log_3_20180715_000000000000.csv.gz', 'gs://ai-project-2020-spotify/training_set/log_4_20180715_000000000000.csv.gz', 'gs://ai-project-2020-spotify/training_set/log_5_20180715_000000000000.csv.gz', 'gs://ai-project-2020-spotify/training_set/log_6_20180715_000000000000.csv.gz', 'gs://ai-project-2020-spotify/training_set/log_7_20180715_000000000000.csv.gz']\n"
     ]
    }
   ],
   "source": [
    "# Cloud bucket contains larger datasets:\n",
    "files = []\n",
    "train_files = list(bucket.list_blobs(prefix='training_set/'))\n",
    "for blob in [blob for blob in train_files if '20180715' in blob.name]:\n",
    "  files.append(f\"gs://{bucket_name}/\"+blob.name)\n",
    "  print(blob.name)\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "vHGNPTaL6E3i"
   },
   "outputs": [],
   "source": [
    "# Cloud bucket contains larger datasets:\n",
    "logs = pd.read_csv(f\"gs://{bucket_name}/training_set/log_0_20180715_000000000000.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "burH4XKYDeZA",
    "outputId": "6ef9e3a5-c32a-46d6-d474-dc795fc1f73c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2990609, 44)\n"
     ]
    }
   ],
   "source": [
    "# remove date for convenience (could encode this as well)\n",
    "logs.drop(columns=['date'], inplace=True)\n",
    "\n",
    "# Create dummies (one hot encoding) for each categorical variable in logs\n",
    "categorical_cols = ['context_type', 'hist_user_behavior_reason_start', 'hist_user_behavior_reason_end']\n",
    "logs = categorical_to_dummies(logs, categorical_cols)\n",
    "print(logs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lh7laMzw6E3i"
   },
   "source": [
    "## Import Track Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ZmMaN1NB6E3i"
   },
   "outputs": [],
   "source": [
    "track_features_1 = pd.read_csv(f\"gs://{bucket_name}/track_features/tf_000000000000.csv\").set_index('track_id')\n",
    "track_features_2 = pd.read_csv(f\"gs://{bucket_name}/track_features/tf_000000000001.csv\").set_index('track_id')\n",
    "track_features = track_features_1.append(track_features_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "gLO1_HpUEojT"
   },
   "outputs": [],
   "source": [
    "track_features = categorical_to_dummies(track_features, ['mode'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4ie4j2YPNp0"
   },
   "source": [
    "## Determine Model Shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "ag9lM_j_2i6j"
   },
   "outputs": [],
   "source": [
    "def find_cat_user_behavior(files):\n",
    "  ''' Will find all categorical values for the columns 'hist_user_behavior_reason_start', 'hist_user_behavior_reason_end', 'context_type'.\n",
    "  args: \n",
    "    files: list of csv files to get categorical values from\n",
    "  returns:\n",
    "    hist_user_behavior_reason_end_cat: name of categorical columns for hist_user_behavior_reason_end\n",
    "    hist_user_behavior_reason_cat: name of categorical columns for hist_user_behavior_reason_start\n",
    "    context_type_cat: name of categorical columns for context_type\n",
    "  '''\n",
    "  iterator_generator = (pd.read_csv(f, usecols=['hist_user_behavior_reason_start', 'hist_user_behavior_reason_end', 'context_type']) for f in files)\n",
    "  hist_user_behavior_reason_cat = set()\n",
    "  hist_user_behavior_reason_end_cat = set()\n",
    "  context_type_cat = set()\n",
    "  for iterator in iterator_generator:\n",
    "    for col in iterator['hist_user_behavior_reason_start'].unique():\n",
    "      hist_user_behavior_reason_cat.add('hist_user_behavior_reason_start_'+col)\n",
    "    for col in iterator['hist_user_behavior_reason_end'].unique():\n",
    "      hist_user_behavior_reason_end_cat.add('hist_user_behavior_reason_end_'+col)\n",
    "    for col in iterator['context_type'].unique():\n",
    "        context_type_cat.add('context_type_'+col)\n",
    "  return hist_user_behavior_reason_end_cat, hist_user_behavior_reason_cat, context_type_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Q-D-cfpE4Mqv"
   },
   "outputs": [],
   "source": [
    "hist_user_behavior_reason_end_cat, hist_user_behavior_reason_cat, context_type_cat = find_cat_user_behavior(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "YJZv0UohE3Mt"
   },
   "outputs": [],
   "source": [
    "# All track feature columns:\n",
    "track_features_columns = set(track_features.columns.tolist())\n",
    "# All log columns:\n",
    "logs = pd.read_csv(files[0])\n",
    "log_columns = set(logs.columns.tolist())\n",
    "log_columns = hist_user_behavior_reason_end_cat.union(hist_user_behavior_reason_cat).union(context_type_cat).union(log_columns)\n",
    "# Columns we won't input into the model but that are present in the logs:\n",
    "unwanted_columns = {'session_id','session_position', 'session_length', 'track_id_clean', 'date',\n",
    "       'skip_1', 'skip_2', 'skip_3', 'not_skipped', 'context_type', 'hist_user_behavior_reason_start', 'hist_user_behavior_reason_end'}\n",
    "# Columns you have access to in first part of session: all track feature columns + log columns - unwanted_columns\n",
    "first_part_session_columns = list(track_features_columns.union(log_columns).difference(unwanted_columns))\n",
    "# Columns you have access to in second part of session: all track feature columns\n",
    "second_part_session_columns = list(track_features_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3FFSI1JAaARr",
    "outputId": "b15cd903-a3fd-4a4f-a32e-8cc226ac9a11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hist_user_behavior_reason_end_clickrow', 'mechanism', 'context_type_catalog', 'duration', 'hour_of_day', 'context_type_charts', 'hist_user_behavior_reason_end_clickside', 'context_type_user_collection', 'acousticness', 'energy', 'beat_strength', 'no_pause_before_play', 'hist_user_behavior_is_shuffle', 'hist_user_behavior_reason_end_backbtn', 'hist_user_behavior_reason_end_endplay', 'hist_user_behavior_reason_start_playbtn', 'context_type_personalized_playlist', 'acoustic_vector_0', 'hist_user_behavior_reason_start_trackdone', 'hist_user_behavior_reason_start_endplay', 'hist_user_behavior_reason_start_clickside', 'hist_user_behavior_reason_start_trackerror', 'bounciness', 'organism', 'context_type_editorial_playlist', 'hist_user_behavior_reason_end_remote', 'mode_major', 'release_year', 'flatness', 'hist_user_behavior_reason_start_uriopen', 'long_pause_before_play', 'acoustic_vector_1', 'acoustic_vector_3', 'time_signature', 'hist_user_behavior_reason_start_remote', 'hist_user_behavior_reason_start_popup', 'valence', 'danceability', 'premium', 'hist_user_behavior_n_seekback', 'mode_minor', 'hist_user_behavior_reason_end_popup', 'short_pause_before_play', 'acoustic_vector_7', 'key', 'acoustic_vector_4', 'dyn_range_mean', 'hist_user_behavior_reason_start_clickrow', 'hist_user_behavior_n_seekfwd', 'speechiness', 'hist_user_behavior_reason_start_appload', 'context_type_radio', 'acoustic_vector_5', 'hist_user_behavior_reason_end_fwdbtn', 'hist_user_behavior_reason_start_backbtn', 'liveness', 'acoustic_vector_2', 'instrumentalness', 'hist_user_behavior_reason_end_uriopen', 'loudness', 'hist_user_behavior_reason_end_logout', 'acoustic_vector_6', 'hist_user_behavior_reason_end_appload', 'hist_user_behavior_reason_end_trackdone', 'context_switch', 'us_popularity_estimate', 'tempo', 'hist_user_behavior_reason_start_fwdbtn']\n"
     ]
    }
   ],
   "source": [
    "print(first_part_session_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "29pTrpfOQvgc",
    "outputId": "c6a16a47-3484-4b96-a610-b5b35775176b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['time_signature', 'mechanism', 'duration', 'acousticness', 'energy', 'beat_strength', 'valence', 'danceability', 'mode_minor', 'acoustic_vector_7', 'key', 'acoustic_vector_4', 'dyn_range_mean', 'acoustic_vector_0', 'speechiness', 'acoustic_vector_5', 'bounciness', 'organism', 'liveness', 'release_year', 'instrumentalness', 'acoustic_vector_2', 'mode_major', 'flatness', 'loudness', 'acoustic_vector_6', 'acoustic_vector_1', 'us_popularity_estimate', 'acoustic_vector_3', 'tempo']\n"
     ]
    }
   ],
   "source": [
    "print(second_part_session_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZLIVNjRrIgCc",
    "outputId": "ca30794d-c21a-4dc9-92a6-e994c2c7eed5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(first_part_session_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NMguId8pJgUO",
    "outputId": "c292a68c-c0d6-4f5e-cd31-a65f3a0bccb2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(second_part_session_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HjD7sMYVBRBZ"
   },
   "source": [
    "## Data Wrangling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "LV1VJOdA6E3j"
   },
   "outputs": [],
   "source": [
    "def create_matrix(data):\n",
    "    # print(\" ## Splitting into First and Second Part\")\n",
    "    data1 = data[data.session_position <= (data.session_length / 2)]\n",
    "    data2 = data[data.session_position > (data.session_length / 2)]\n",
    "    \n",
    "    # Split into first and second part\n",
    "    start_sessions = data1.groupby(\"session_id\")\n",
    "    end_sessions = data2.groupby(\"session_id\")\n",
    "    \n",
    "    X1, X2, y = [], [], []\n",
    "    \n",
    "    # For first part\n",
    "    # print(\" ## Processing First Part of Sessions\")\n",
    "    for session_id in start_sessions.groups.keys():\n",
    "        # Get columns based on group\n",
    "        session = start_sessions.get_group(session_id)[first_part_session_columns + [\"skip_2\"]]\n",
    "        # Set skip_2\n",
    "        session[\"skip_2\"] = session[\"skip_2\"] * 1\n",
    "        x = session.to_numpy()\n",
    "        # Padding operation\n",
    "        X1.append(np.pad(x, ((0, 10 - len(x)), (0, 0)), 'constant', constant_values=(0)))\n",
    "    \n",
    "    # For second part\n",
    "    # print(\" ## Processing Second Part of Sessions\")\n",
    "    for session_id in end_sessions.groups.keys():\n",
    "        session = end_sessions.get_group(session_id)[second_part_session_columns + [\"skip_2\"]]\n",
    "        # Set \n",
    "        true_y = session[\"skip_2\"].to_numpy() * 1\n",
    "        # remove skip_2 from second part\n",
    "        del session[\"skip_2\"]\n",
    "        x = session.to_numpy()\n",
    "        X2.append(np.pad(x, ((0, 10 - len(x)), (0, 0)), 'constant', constant_values=(0)))\n",
    "        y.append(np.pad(true_y, (0, 10 - len(true_y)), 'constant', constant_values=(0)))\n",
    "    \n",
    "    X1 = np.array(X1)\n",
    "    X2 = np.array(X2)\n",
    "    y = np.array(y)\n",
    "    return X1,X2,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "p8AzvWLk58s-"
   },
   "outputs": [],
   "source": [
    "def logs_categorical_columns_adder(data):\n",
    "  ''' \n",
    "  Some chunks don't have the same categorical values for hist_user_behavior_reason_end and hist_user_behavior_reason_cat. \n",
    "  This function will add empty one-hot-encoded columns to dataframes.\n",
    "\n",
    "  args:\n",
    "    data: dataframe to add categorical values to.\n",
    "\n",
    "  returns:\n",
    "    data: dataframe with new empty one-hot-encoded columns.\n",
    "  '''\n",
    "  col_to_add = hist_user_behavior_reason_end_cat.union(hist_user_behavior_reason_cat).difference(set(data.columns))\n",
    "  for col in col_to_add:\n",
    "    data[col] = 0\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "2Km3VwS5FS2p"
   },
   "outputs": [],
   "source": [
    "def logs_cleaning(data):\n",
    "  ''' Cleans data. \n",
    "  args:\n",
    "    data: dataframe to clean.\n",
    "  returns:\n",
    "    data: cleaned dataframe.\n",
    "  '''\n",
    "  # remove date for convenience (could encode this as well)\n",
    "  data.drop(columns=['date'], inplace=True)\n",
    "  # Create dummies (one hot encoding) for each categorical variable in logs\n",
    "  categorical_cols = ['context_type', 'hist_user_behavior_reason_start', 'hist_user_behavior_reason_end']\n",
    "  data = categorical_to_dummies(data, categorical_cols)\n",
    "\n",
    "  # Convert booleans to ints\n",
    "  data['premium'] = data['premium']*1\n",
    "  data['hist_user_behavior_is_shuffle'] = data['hist_user_behavior_is_shuffle']*1\n",
    "  data['skip_1'] = data['skip_1']*1\n",
    "  data['skip_2'] = data['skip_2']*1\n",
    "  data['skip_3'] = data['skip_3']*1\n",
    "\n",
    "  # Normalize\n",
    "  feature_name = ['duration',\n",
    "  'release_year',\n",
    "  'us_popularity_estimate',\n",
    "  'flatness',\n",
    "  'loudness',\n",
    "  'tempo',\n",
    "  'acoustic_vector_0',\n",
    "  'acoustic_vector_1',\n",
    "  'acoustic_vector_2',\n",
    "  'acoustic_vector_3',\n",
    "  'acoustic_vector_4',\n",
    "  'acoustic_vector_5',\n",
    "  'acoustic_vector_6',\n",
    "  'acoustic_vector_7',\n",
    "  'key']\n",
    "\n",
    "  data = normalize(data, feature_name)\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "oXz-od51FhwC"
   },
   "outputs": [],
   "source": [
    "def logs_feature_joining(data):\n",
    "  ''' Joins a chunk of data from the session logs with the track features. \n",
    "  args:\n",
    "    data: dataframe to join features to.\n",
    "  returns:\n",
    "    data: dataframe including logs and track feature columns.\n",
    "  '''\n",
    "  data = data.join(track_features, on='track_id_clean', how='left')\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "w8IT5yKllX4e"
   },
   "outputs": [],
   "source": [
    "def data_generator(files, chunksize=1000):\n",
    "  ''' Will infinitely generate chunks of data from all csv files.\n",
    "  args:\n",
    "    files: list of path names to CSV files holding session logs.\n",
    "    chunksize: CSV files will be read in chunks of size chunksize.\n",
    "\n",
    "  Note:\n",
    "    Chunk size isn't uniform, will only return full sessions.\n",
    "    So, one chunk might be of length 997, next might be 1005.\n",
    "    Reason being that chunks can split a session in two, this is unwanted behavior.\n",
    "  '''\n",
    "  assert isinstance(files, list), \"files argument should be list of paths\"\n",
    "  while True:\n",
    "    iterator_generator = (pd.read_csv(f, iterator=True, chunksize=chunksize) for f in files)\n",
    "    dummy = pd.DataFrame()\n",
    "    for iterator in iterator_generator:\n",
    "      # print('\\n### Opened new file')\n",
    "      for chunk in iterator:\n",
    "        # Get position of last row element\n",
    "        last_position = chunk.iloc[-1]['session_position']\n",
    "        # Slice last session\n",
    "        last_session = chunk.iloc[-last_position:]\n",
    "        # Drop last session from current chunk\n",
    "        chunk.drop(last_session.index, inplace=True)\n",
    "        # Append chunk to previous last session (to get a full session)\n",
    "        dummy = dummy.append(chunk)\n",
    "        yield dummy\n",
    "        # Assign this last session to dummy for next session\n",
    "        dummy = last_session\n",
    "  print('\\n### Processed all Files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "96kFq_0L5W0j"
   },
   "outputs": [],
   "source": [
    "def batch(iterables_list, batch_size=1):\n",
    "  ''' Will return batches of a iterables.\n",
    "  args:\n",
    "    iterables_list: list of iterables to return batches from.\n",
    "    batch_size: size of batch.\n",
    "  \n",
    "  Note:\n",
    "    Will always return a batch of batch_size, even if iterables list is smaller than batch_size.\n",
    "    Does this by mean of addings duplicates.\n",
    "  '''\n",
    "  if iterables_list[0].shape[0] != iterables_list[1].shape[0] != iterables_list[2].shape[0]:\n",
    "    raise ValueError\n",
    "  l = len(iterables_list[0])\n",
    "  for ndx in range(0, l, batch_size):\n",
    "      if min(ndx + batch_size, l) == l:\n",
    "        yield iterables_list[0][l-batch_size: l], iterables_list[1][l-batch_size: l], iterables_list[2][l-batch_size: l]\n",
    "      else: \n",
    "        yield iterables_list[0][ndx:min(ndx + batch_size, l)], iterables_list[1][ndx:min(ndx + batch_size, l)], iterables_list[2][ndx:min(ndx + batch_size, l)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "vhGAY3pB4j9R"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "tSfa1Nd9PxG0"
   },
   "outputs": [],
   "source": [
    "def cosineSimilarity(data):\n",
    "  ''' Will calculate the cosine similarity between \n",
    "  1. skipped songs and current song\n",
    "  2. songs in the second half of the data and current song '''\n",
    "\n",
    "  skipped_songs_data = data[data['skip_2'] == 1]\n",
    "  second_half_songs_data = data[data['session_position'] > 0.5 * data['session_length']]\n",
    "\n",
    "  # Option 1\n",
    "  cosine_distance_columns = ['duration', 'release_year', 'us_popularity_estimate', 'acousticness',\n",
    "       'beat_strength', 'bounciness', 'danceability', 'dyn_range_mean',\n",
    "       'energy', 'flatness', 'instrumentalness', 'key', 'liveness', 'loudness',\n",
    "       'mechanism', 'organism', 'speechiness', 'tempo', 'time_signature',\n",
    "       'valence', 'acoustic_vector_0', 'acoustic_vector_1',\n",
    "       'acoustic_vector_2', 'acoustic_vector_3', 'acoustic_vector_4',\n",
    "       'acoustic_vector_5', 'acoustic_vector_6', 'acoustic_vector_7']\n",
    "  \n",
    "  # Option 2\n",
    "  #cosine_distance_columns = ['acoustic_vector_0', 'acoustic_vector_1',\n",
    "  #     'acoustic_vector_2', 'acoustic_vector_3', 'acoustic_vector_4',\n",
    "  #     'acoustic_vector_5', 'acoustic_vector_6', 'acoustic_vector_7']\n",
    "\n",
    "  # Option 3\n",
    "  # cosine_distance_columns = ['acousticness',\n",
    "  #     'beat_strength', 'bounciness', 'danceability', 'dyn_range_mean',\n",
    "  #     'energy', 'flatness', 'instrumentalness', 'key', 'liveness', 'loudness',\n",
    "  #     'mechanism', 'organism', 'speechiness', 'tempo', 'time_signature',\n",
    "  #     'valence']\n",
    "  \n",
    "  #skipped_songs_data[cosine_distance_columns]\n",
    "  #second_half_songs_data[cosine_distance_columns]\n",
    "\n",
    "  mean_skipped_song = skipped_songs_data[cosine_distance_columns].mean().tolist()\n",
    "  mean_second_half_songs = second_half_songs_data[cosine_distance_columns].mean().tolist()\n",
    "\n",
    "  tqdm.pandas()\n",
    "\n",
    "  data['similarity_mean_skipped_song'] = data[cosine_distance_columns].apply(lambda x: cosine_similarity([mean_skipped_song], [x.tolist()])[0][0], axis=1)\n",
    "\n",
    "  data['similarity_mean_second_half_songs'] = data[cosine_distance_columns].apply(lambda x: cosine_similarity([mean_second_half_songs], [x.tolist()])[0][0], axis=1)\n",
    "\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "7__Lz6XQsnVI"
   },
   "outputs": [],
   "source": [
    "def data_processor(files, batchsize=100, chunksize=10000):\n",
    "  ''' Generator: will clean, join,... each chunk.\n",
    "  args:\n",
    "    files: list of path names to CSV files holding session logs.\n",
    "    chunksize: CSV files will be read in chunks of size chunksize.\n",
    "    batch_size: size of batch to yield.\n",
    "  \n",
    "  yields: tuple of (X1_batch, X2_batch), y_batch. Where y_batch are target values.\n",
    "    X1_batch, X2_batch are the first and second session part with size = batch_size.\n",
    "  '''\n",
    "  generator = data_generator(files, chunksize)\n",
    "  amount_of_sessions = 0\n",
    "  \n",
    "  for chunk in generator:\n",
    "    amount_of_sessions += chunk['session_id'].nunique()\n",
    "    chunk = logs_feature_joining(chunk)\n",
    "    chunk = logs_cleaning(chunk)\n",
    "    chunk = logs_categorical_columns_adder(chunk)\n",
    "    chunk = cosineSimilarity(chunk)\n",
    "    X1, X2, y = create_matrix(chunk)\n",
    "    for X1_batch, X2_batch, y_batch in batch([X1,X2,y], batchsize):\n",
    "          yield (X1_batch, X2_batch), y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "gZctuLOIzi6v"
   },
   "outputs": [],
   "source": [
    "# Pre-processing data\n",
    "def data_pre_processor(files, filenameX1, filenameX2, filenamey, batchsize=100, chunksize=20000, delete=False):\n",
    "  ''' Pipeline: will clean, join,... each chunk and save to csv.\n",
    "  args:\n",
    "    files (list): list of path names to CSV files holding session logs.\n",
    "    chunksize (int): CSV files will be read in chunks of size chunksize.\n",
    "    filename (string): path to save file to.\n",
    "    delete (bool): delete file at filename first or not.\n",
    "  '''\n",
    "  generator = data_generator(files, chunksize)\n",
    "  amount_of_sessions = 0\n",
    "  header = True\n",
    "  if delete:\n",
    "    os.remove(filename)\n",
    "  for chunk in generator:\n",
    "    amount_of_sessions += chunk['session_id'].nunique()\n",
    "    chunk = logs_feature_joining(chunk)\n",
    "    chunk = logs_cleaning(chunk)\n",
    "    chunk = logs_categorical_columns_adder(chunk)\n",
    "    chunk = cosineSimilarity(chunk)\n",
    "    chunk.to_csv(filename, header=True, mode='a')\n",
    "    header = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "6hJ2_NSq5gaw"
   },
   "outputs": [],
   "source": [
    "def X_y(data, batchsize = 100):\n",
    "  X1, X2, y = create_matrix(data)\n",
    "  for X1_batch, X2_batch, y_batch in batch([X1,X2,y], batchsize):\n",
    "        yield (X1_batch, X2_batch), y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gok6EKDBXFXV"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "uxGUiZtG6E3j"
   },
   "outputs": [],
   "source": [
    "# Network 1 - First half of the session\n",
    "input_layer1 = layers.Input(shape=(10, len(first_part_session_columns)+1))\n",
    "\n",
    "# Normalisation\n",
    "norm1 = layers.BatchNormalization()(input_layer1)\n",
    "\n",
    "# Recurrent layer(s)\n",
    "lstm1 = layers.Bidirectional(layers.GRU(25, return_sequences=False, input_shape=(10, len(first_part_session_columns)+1)))(norm1)\n",
    "\n",
    "# Network 2 - Second half of the session\n",
    "input_layer2 = layers.Input(shape=(10, len(second_part_session_columns)))\n",
    "\n",
    "# Normalisation\n",
    "norm2 = layers.BatchNormalization()(input_layer2)\n",
    "\n",
    "# Recurrent layer(s)\n",
    "lstm2 = layers.Bidirectional(layers.GRU(25, return_sequences=False, input_shape=(10, len(second_part_session_columns))))(norm2)\n",
    "\n",
    "# Concatenation & dense layer\n",
    "concat = layers.Concatenate()([lstm1, lstm2])\n",
    "dense_last = layers.Dense(10, activation=\"relu\")(concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "stUqO0PrUo9x"
   },
   "outputs": [],
   "source": [
    "def calculate_steps_per_epoch(files, batchsize):\n",
    "  # Calculates lower limit of how many batches there will be.\n",
    "  iterator_generator = (pd.read_csv(f, usecols=['session_id']) for f in files)\n",
    "  unique_session_count = 0\n",
    "  for iterator in iterator_generator:\n",
    "    unique_session_count += iterator['session_id'].nunique()\n",
    "  steps_per_epoch = unique_session_count / batchsize\n",
    "  return int(steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MMwzw5_fVvFN",
    "outputId": "bf1ae116-8535-487d-a806-4616becc69ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14232\n"
     ]
    }
   ],
   "source": [
    "steps = calculate_steps_per_epoch(files, 100)\n",
    "print(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lhDZMnMx6E3j",
    "outputId": "6607b3cd-f8ce-402a-dadc-75da8f1c020f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/std.py:703: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "    4/14232 [..............................] - ETA: 3:04 - loss: 0.4283 - acc: 0.0950"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/std.py:703: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   13/14232 [..............................] - ETA: 3:13:39 - loss: 0.4173 - acc: 0.1862"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/std.py:703: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   18/14232 [..............................] - ETA: 4:35:48 - loss: 0.4124 - acc: 0.2261"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/std.py:703: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   23/14232 [..............................] - ETA: 5:22:02 - loss: 0.4148 - acc: 0.2339"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/std.py:703: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   30/14232 [..............................] - ETA: 5:29:46 - loss: 0.4096 - acc: 0.2393"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/std.py:703: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   35/14232 [..............................] - ETA: 5:53:05 - loss: 0.4060 - acc: 0.2374"
     ]
    }
   ],
   "source": [
    "# Compile\n",
    "lossf = keras.losses.MeanAbsoluteError()\n",
    "model = keras.Model(inputs=[input_layer1, input_layer2], outputs=[dense_last])\n",
    "model.compile(optimizer='adam', loss=lossf, metrics=[\"acc\"])\n",
    "\n",
    "processor = data_processor(files, batchsize=100, chunksize=10000)\n",
    "history = model.fit(processor, epochs=5, steps_per_epoch=steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ha4s-WfowwHh"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "znyEB2jnmyAB"
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "steps = calculate_steps_per_epoch([files[7]], 100)\n",
    "processor = data_processor([files[7]], batchsize=100, chunksize=10000)\n",
    "\n",
    "truth = []\n",
    "predictions = []\n",
    "\n",
    "for step in tqdm(range(steps)):\n",
    "  X_batches, y_batch = processor.__next__()\n",
    "  prediction = model.predict(X_batches)\n",
    "  predictions.extend(prediction)\n",
    "  truth.extend(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QXLZ2CuhFrUK"
   },
   "outputs": [],
   "source": [
    "evaluate([[i >= 0.5 for i in p] for p in predictions], truth)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "rnn_jens_cos_sim.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "common-cpu.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
