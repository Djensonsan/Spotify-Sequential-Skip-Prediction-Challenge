{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GBT.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Fhly5s4hqtvc",
        "yR60FBXsqzAF",
        "NI6oAUSCq5A4",
        "FtlgoEkusSu3",
        "YJ7royKpvOdd",
        "AOhvw0AZrLMy"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNwlzuFZ1fCSvfor09bsam1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Djensonsan/Spotify-Sequential-Skip-Prediction-Challenge/blob/main/GBT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LM2F6nQAp_xb"
      },
      "source": [
        "# GBT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyhHuspUqFNQ"
      },
      "source": [
        "## Imports and Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOPvrQExp73D",
        "outputId": "aac11d3a-e2ed-4519-b699-69e0899f2633"
      },
      "source": [
        "# Install your required packages here\n",
        "!pip install pandas numpy matplotlib sklearn fsspec gcsfs tqdm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.2.2)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.6/dist-packages (0.8.4)\n",
            "Requirement already satisfied: gcsfs in /usr/local/lib/python3.6/dist-packages (0.7.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.6/dist-packages (from gcsfs) (1.17.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gcsfs) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.6/dist-packages (from gcsfs) (0.4.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.6/dist-packages (from gcsfs) (3.7.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from gcsfs) (4.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (0.17.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (50.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (4.1.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (4.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (1.24.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib->gcsfs) (1.3.0)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->gcsfs) (3.0.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->gcsfs) (1.6.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->gcsfs) (20.3.0)\n",
            "Requirement already satisfied: idna-ssl>=1.0; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp->gcsfs) (1.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.6/dist-packages (from aiohttp->gcsfs) (3.7.4.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.6/dist-packages (from aiohttp->gcsfs) (5.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnCh58DaqJRi",
        "outputId": "6d8e3562-8bd9-4a88-aae8-bf8a1d515511"
      },
      "source": [
        "# Path to credentials for cloud bucket:\n",
        "%env GOOGLE_APPLICATION_CREDENTIALS=/content/drive/My Drive/CS/AI/Credentials/ai-project-2020-f4dfbc25326c.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: GOOGLE_APPLICATION_CREDENTIALS=/content/drive/My Drive/CS/AI/Credentials/ai-project-2020-f4dfbc25326c.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IwiAJ4CqKSZ",
        "outputId": "1a42f46f-15c8-48a9-9924-f1cff4b4881d"
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "import sklearn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.python.keras.utils.vis_utils import plot_model\n",
        "\n",
        "from glob import glob\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from google.cloud import storage\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CY0LrkywqLZE"
      },
      "source": [
        "pd.set_option('display.max_rows', 800)\n",
        "pd.set_option('display.max_columns', 800)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3HnS9lhqoLW"
      },
      "source": [
        "bucket_name = \"ai-project-2020-spotify\"\n",
        "client = storage.Client()\n",
        "bucket = client.get_bucket(bucket_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fhly5s4hqtvc"
      },
      "source": [
        "## Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JauOH-jFqvQR"
      },
      "source": [
        "def ave_pre(submission, groundtruth):\n",
        "    \"\"\" Calculate average accuracy (which is the same as average precision in this context) \"\"\"\n",
        "    s = 0.0\n",
        "    t = 0.0\n",
        "    c = 1.0\n",
        "    for x, y in zip(submission, groundtruth):\n",
        "        if x != 0 and x != 1:\n",
        "            raise ValueError()\n",
        "        if x == y:\n",
        "            s += 1.0\n",
        "            t += s / c\n",
        "        c += 1\n",
        "    return t / len(groundtruth)\n",
        "\n",
        "def evaluate(submission, groundtruth):\n",
        "    \"\"\" Calculate metrics for prediction and ground thruth lists (source: starter kit) \"\"\"\n",
        "    ap_sum = 0.0\n",
        "    first_pred_acc_sum = 0.0\n",
        "    counter = 0\n",
        "    for sub, tru in zip(submission, groundtruth):\n",
        "        # if len(sub) != len(tru):\n",
        "        #     raise Exception('Line {} should contain {} predictions, but instead contains '\n",
        "        #                     '{}'.format(counter + 1, len(tru), len(sub)))\n",
        "        try:\n",
        "            ap_sum += ave_pre(sub, tru)\n",
        "        except ValueError as e:\n",
        "            raise ValueError('Invalid prediction in line {}, should be 0 or 1'.format(counter))\n",
        "        first_pred_acc_sum += sub[0] == tru[0]\n",
        "        counter += 1\n",
        "    ap = ap_sum / counter\n",
        "    first_pred_acc = first_pred_acc_sum / counter\n",
        "    return ap, first_pred_acc\n",
        "\n",
        "def normalize(df,feature_name):\n",
        "    result = df.copy()\n",
        "    for name in feature_name:\n",
        "        max_value = df[name].max()\n",
        "        min_value = df[name].min()\n",
        "        result[name] = (df[name] - min_value) / (max_value - min_value)\n",
        "    return result\n",
        "\n",
        "def categorical_to_dummies(df, categorical_cols):\n",
        "    \"\"\" Create dummies (one hot encoding) for each categorical variables \"\"\"\n",
        "    dummies = pd.get_dummies(df[categorical_cols], prefix=categorical_cols)\n",
        "    return df.drop(columns=categorical_cols).join(dummies)\n",
        "\n",
        "def split_sessions(data, perc_in=0.6):\n",
        "    \"\"\" Split interactions into train and test sessions. \"\"\"\n",
        "    sessions = data['session_id'].unique()\n",
        "    amt_in = int(perc_in * len(sessions))\n",
        "    sessions_in = np.random.choice(sessions, amt_in, replace=False)\n",
        "    sessions_out = np.array(list(set(sessions) - set(sessions_in)))\n",
        "    indexed_data = data.set_index('session_id')\n",
        "    data_in = indexed_data.loc[sessions_in]\n",
        "    data_out = indexed_data.loc[sessions_out]\n",
        "    return data_in, data_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR60FBXsqzAF"
      },
      "source": [
        "## Import Session Logs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_tXKYFuqv5a",
        "outputId": "0a8f2c49-d360-4753-bd20-50498e80a69a"
      },
      "source": [
        "# Cloud bucket contains larger datasets:\n",
        "files = []\n",
        "train_files = list(bucket.list_blobs(prefix='training_set/'))\n",
        "for blob in [blob for blob in train_files if '20180715' in blob.name]:\n",
        "  files.append(f\"gs://{bucket_name}/\"+blob.name)\n",
        "  print(blob.name)\n",
        "print(files)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training_set/log_0_20180715_000000000000.csv.gz\n",
            "training_set/log_1_20180715_000000000000.csv.gz\n",
            "training_set/log_2_20180715_000000000000.csv.gz\n",
            "training_set/log_3_20180715_000000000000.csv.gz\n",
            "training_set/log_4_20180715_000000000000.csv.gz\n",
            "training_set/log_5_20180715_000000000000.csv.gz\n",
            "training_set/log_6_20180715_000000000000.csv.gz\n",
            "training_set/log_7_20180715_000000000000.csv.gz\n",
            "['gs://ai-project-2020-spotify/training_set/log_0_20180715_000000000000.csv.gz', 'gs://ai-project-2020-spotify/training_set/log_1_20180715_000000000000.csv.gz', 'gs://ai-project-2020-spotify/training_set/log_2_20180715_000000000000.csv.gz', 'gs://ai-project-2020-spotify/training_set/log_3_20180715_000000000000.csv.gz', 'gs://ai-project-2020-spotify/training_set/log_4_20180715_000000000000.csv.gz', 'gs://ai-project-2020-spotify/training_set/log_5_20180715_000000000000.csv.gz', 'gs://ai-project-2020-spotify/training_set/log_6_20180715_000000000000.csv.gz', 'gs://ai-project-2020-spotify/training_set/log_7_20180715_000000000000.csv.gz']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NI6oAUSCq5A4"
      },
      "source": [
        "## Import Track Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayGJMHHCq-xs"
      },
      "source": [
        "track_features_1 = pd.read_csv('/content/drive/My Drive/CS/AI/Data/track_features/tf_000000000000.csv').set_index('track_id')\n",
        "track_features_2 = pd.read_csv('/content/drive/My Drive/CS/AI/Data/track_features/tf_000000000001.csv').set_index('track_id')\n",
        "track_features = track_features_1.append(track_features_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpRikh_LrA6B"
      },
      "source": [
        "track_features = categorical_to_dummies(track_features, ['mode'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtlgoEkusSu3"
      },
      "source": [
        "## Track Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4y-jyOhrr6I"
      },
      "source": [
        "def duration_binning(track_features):\n",
        "  ''' Bin the duration of each track. \n",
        "  '''\n",
        "  cut_bins = [0, 60, 90, 120, 150, 180, 210, 240, 270, 300, 600, 999999]\n",
        "  bin_names = ['60', '90', '120', '150', '180', '210', '240', '270', '300', 'long', 'very_long']\n",
        "  track_features['duration'] = pd.cut(track_features['duration'], bins=cut_bins, labels=bin_names)\n",
        "  track_features = categorical_to_dummies(track_features, ['duration'])\n",
        "  return track_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTuQPnJXsY9q"
      },
      "source": [
        "def decades_binning(track_features):\n",
        "  cut_bins = [0, 1960, 1970, 1980, 1990, 2000, 2010, 2020]\n",
        "  bin_names = ['60s', '70s', '80s', '90s', '00s', '10s', '20s']\n",
        "  track_features['release_year'] = pd.cut(track_features['release_year'], bins=cut_bins, labels=bin_names)\n",
        "  track_features = categorical_to_dummies(track_features, ['release_year'])\n",
        "  return track_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkPX0SdPvFeg"
      },
      "source": [
        "track_features = duration_binning(track_features)\n",
        "track_features = decades_binning(track_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJ7royKpvOdd"
      },
      "source": [
        "### Sanity Check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEuQtTkkvPyt",
        "outputId": "77cec663-e3bd-4cc2-9e0e-beb096f8d719"
      },
      "source": [
        "track_features.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>us_popularity_estimate</th>\n",
              "      <th>acousticness</th>\n",
              "      <th>beat_strength</th>\n",
              "      <th>bounciness</th>\n",
              "      <th>danceability</th>\n",
              "      <th>dyn_range_mean</th>\n",
              "      <th>energy</th>\n",
              "      <th>flatness</th>\n",
              "      <th>instrumentalness</th>\n",
              "      <th>key</th>\n",
              "      <th>liveness</th>\n",
              "      <th>loudness</th>\n",
              "      <th>mechanism</th>\n",
              "      <th>organism</th>\n",
              "      <th>speechiness</th>\n",
              "      <th>tempo</th>\n",
              "      <th>time_signature</th>\n",
              "      <th>valence</th>\n",
              "      <th>acoustic_vector_0</th>\n",
              "      <th>acoustic_vector_1</th>\n",
              "      <th>acoustic_vector_2</th>\n",
              "      <th>acoustic_vector_3</th>\n",
              "      <th>acoustic_vector_4</th>\n",
              "      <th>acoustic_vector_5</th>\n",
              "      <th>acoustic_vector_6</th>\n",
              "      <th>acoustic_vector_7</th>\n",
              "      <th>mode_major</th>\n",
              "      <th>mode_minor</th>\n",
              "      <th>duration_60</th>\n",
              "      <th>duration_90</th>\n",
              "      <th>duration_120</th>\n",
              "      <th>duration_150</th>\n",
              "      <th>duration_180</th>\n",
              "      <th>duration_210</th>\n",
              "      <th>duration_240</th>\n",
              "      <th>duration_270</th>\n",
              "      <th>duration_300</th>\n",
              "      <th>duration_long</th>\n",
              "      <th>duration_very_long</th>\n",
              "      <th>release_year_60s</th>\n",
              "      <th>release_year_70s</th>\n",
              "      <th>release_year_80s</th>\n",
              "      <th>release_year_90s</th>\n",
              "      <th>release_year_00s</th>\n",
              "      <th>release_year_10s</th>\n",
              "      <th>release_year_20s</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>track_id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>t_2e8f4b71-8a0b-4b9c-b7d8-fb5208e87f9f</th>\n",
              "      <td>99.582885</td>\n",
              "      <td>0.716209</td>\n",
              "      <td>0.366495</td>\n",
              "      <td>0.332605</td>\n",
              "      <td>0.439835</td>\n",
              "      <td>5.805774</td>\n",
              "      <td>0.238847</td>\n",
              "      <td>1.010700</td>\n",
              "      <td>6.533861e-01</td>\n",
              "      <td>0</td>\n",
              "      <td>0.769258</td>\n",
              "      <td>-17.094</td>\n",
              "      <td>0.199170</td>\n",
              "      <td>0.759699</td>\n",
              "      <td>0.033940</td>\n",
              "      <td>100.370003</td>\n",
              "      <td>4</td>\n",
              "      <td>0.223395</td>\n",
              "      <td>0.146012</td>\n",
              "      <td>-0.706908</td>\n",
              "      <td>0.259496</td>\n",
              "      <td>0.481157</td>\n",
              "      <td>0.238427</td>\n",
              "      <td>-0.098389</td>\n",
              "      <td>-0.254960</td>\n",
              "      <td>-0.227383</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>t_dae2ec0e-ec7b-4b3e-b60c-4a884d0eccb0</th>\n",
              "      <td>97.272035</td>\n",
              "      <td>0.839460</td>\n",
              "      <td>0.362212</td>\n",
              "      <td>0.389829</td>\n",
              "      <td>0.507580</td>\n",
              "      <td>6.845427</td>\n",
              "      <td>0.420476</td>\n",
              "      <td>1.000398</td>\n",
              "      <td>3.941550e-09</td>\n",
              "      <td>0</td>\n",
              "      <td>0.085844</td>\n",
              "      <td>-11.295</td>\n",
              "      <td>0.357639</td>\n",
              "      <td>0.747436</td>\n",
              "      <td>0.049856</td>\n",
              "      <td>141.334000</td>\n",
              "      <td>4</td>\n",
              "      <td>0.484702</td>\n",
              "      <td>0.039554</td>\n",
              "      <td>-0.539554</td>\n",
              "      <td>0.105141</td>\n",
              "      <td>0.692589</td>\n",
              "      <td>0.226047</td>\n",
              "      <td>-0.468162</td>\n",
              "      <td>0.164389</td>\n",
              "      <td>-0.769024</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>t_cf0164dd-1531-4399-bfa6-dec19cd1fedc</th>\n",
              "      <td>99.620384</td>\n",
              "      <td>0.054673</td>\n",
              "      <td>0.495002</td>\n",
              "      <td>0.589378</td>\n",
              "      <td>0.552311</td>\n",
              "      <td>9.361949</td>\n",
              "      <td>0.842938</td>\n",
              "      <td>0.957766</td>\n",
              "      <td>1.041595e-01</td>\n",
              "      <td>0</td>\n",
              "      <td>0.407325</td>\n",
              "      <td>-9.310</td>\n",
              "      <td>0.304721</td>\n",
              "      <td>0.493154</td>\n",
              "      <td>0.071753</td>\n",
              "      <td>138.889999</td>\n",
              "      <td>4</td>\n",
              "      <td>0.818441</td>\n",
              "      <td>0.083863</td>\n",
              "      <td>-0.242108</td>\n",
              "      <td>-0.014258</td>\n",
              "      <td>0.096396</td>\n",
              "      <td>0.417641</td>\n",
              "      <td>-0.050576</td>\n",
              "      <td>-0.204757</td>\n",
              "      <td>-0.172563</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>t_0f90acc7-d5c5-4e53-901d-55610fbd090c</th>\n",
              "      <td>96.796830</td>\n",
              "      <td>0.042606</td>\n",
              "      <td>0.389634</td>\n",
              "      <td>0.359044</td>\n",
              "      <td>0.585673</td>\n",
              "      <td>6.068578</td>\n",
              "      <td>0.665398</td>\n",
              "      <td>0.947322</td>\n",
              "      <td>1.444963e-05</td>\n",
              "      <td>0</td>\n",
              "      <td>0.251502</td>\n",
              "      <td>-12.159</td>\n",
              "      <td>0.702948</td>\n",
              "      <td>0.212197</td>\n",
              "      <td>0.029425</td>\n",
              "      <td>133.139008</td>\n",
              "      <td>4</td>\n",
              "      <td>0.594829</td>\n",
              "      <td>0.192498</td>\n",
              "      <td>0.340039</td>\n",
              "      <td>0.034846</td>\n",
              "      <td>-0.389794</td>\n",
              "      <td>0.518381</td>\n",
              "      <td>0.185008</td>\n",
              "      <td>-0.079907</td>\n",
              "      <td>-0.016978</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>t_36b9ad02-095a-443d-a697-6c7285d9410a</th>\n",
              "      <td>97.905891</td>\n",
              "      <td>0.249982</td>\n",
              "      <td>0.513640</td>\n",
              "      <td>0.485435</td>\n",
              "      <td>0.635095</td>\n",
              "      <td>7.198735</td>\n",
              "      <td>0.408715</td>\n",
              "      <td>1.014063</td>\n",
              "      <td>5.266880e-01</td>\n",
              "      <td>0</td>\n",
              "      <td>0.218370</td>\n",
              "      <td>-13.813</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>0.193438</td>\n",
              "      <td>0.032178</td>\n",
              "      <td>152.212006</td>\n",
              "      <td>4</td>\n",
              "      <td>0.591289</td>\n",
              "      <td>0.270586</td>\n",
              "      <td>-0.411061</td>\n",
              "      <td>0.165898</td>\n",
              "      <td>0.225652</td>\n",
              "      <td>0.335518</td>\n",
              "      <td>-0.036643</td>\n",
              "      <td>-0.016300</td>\n",
              "      <td>-0.446870</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        us_popularity_estimate  acousticness  \\\n",
              "track_id                                                                       \n",
              "t_2e8f4b71-8a0b-4b9c-b7d8-fb5208e87f9f               99.582885      0.716209   \n",
              "t_dae2ec0e-ec7b-4b3e-b60c-4a884d0eccb0               97.272035      0.839460   \n",
              "t_cf0164dd-1531-4399-bfa6-dec19cd1fedc               99.620384      0.054673   \n",
              "t_0f90acc7-d5c5-4e53-901d-55610fbd090c               96.796830      0.042606   \n",
              "t_36b9ad02-095a-443d-a697-6c7285d9410a               97.905891      0.249982   \n",
              "\n",
              "                                        beat_strength  bounciness  \\\n",
              "track_id                                                            \n",
              "t_2e8f4b71-8a0b-4b9c-b7d8-fb5208e87f9f       0.366495    0.332605   \n",
              "t_dae2ec0e-ec7b-4b3e-b60c-4a884d0eccb0       0.362212    0.389829   \n",
              "t_cf0164dd-1531-4399-bfa6-dec19cd1fedc       0.495002    0.589378   \n",
              "t_0f90acc7-d5c5-4e53-901d-55610fbd090c       0.389634    0.359044   \n",
              "t_36b9ad02-095a-443d-a697-6c7285d9410a       0.513640    0.485435   \n",
              "\n",
              "                                        danceability  dyn_range_mean  \\\n",
              "track_id                                                               \n",
              "t_2e8f4b71-8a0b-4b9c-b7d8-fb5208e87f9f      0.439835        5.805774   \n",
              "t_dae2ec0e-ec7b-4b3e-b60c-4a884d0eccb0      0.507580        6.845427   \n",
              "t_cf0164dd-1531-4399-bfa6-dec19cd1fedc      0.552311        9.361949   \n",
              "t_0f90acc7-d5c5-4e53-901d-55610fbd090c      0.585673        6.068578   \n",
              "t_36b9ad02-095a-443d-a697-6c7285d9410a      0.635095        7.198735   \n",
              "\n",
              "                                          energy  flatness  instrumentalness  \\\n",
              "track_id                                                                       \n",
              "t_2e8f4b71-8a0b-4b9c-b7d8-fb5208e87f9f  0.238847  1.010700      6.533861e-01   \n",
              "t_dae2ec0e-ec7b-4b3e-b60c-4a884d0eccb0  0.420476  1.000398      3.941550e-09   \n",
              "t_cf0164dd-1531-4399-bfa6-dec19cd1fedc  0.842938  0.957766      1.041595e-01   \n",
              "t_0f90acc7-d5c5-4e53-901d-55610fbd090c  0.665398  0.947322      1.444963e-05   \n",
              "t_36b9ad02-095a-443d-a697-6c7285d9410a  0.408715  1.014063      5.266880e-01   \n",
              "\n",
              "                                        key  liveness  loudness  mechanism  \\\n",
              "track_id                                                                     \n",
              "t_2e8f4b71-8a0b-4b9c-b7d8-fb5208e87f9f    0  0.769258   -17.094   0.199170   \n",
              "t_dae2ec0e-ec7b-4b3e-b60c-4a884d0eccb0    0  0.085844   -11.295   0.357639   \n",
              "t_cf0164dd-1531-4399-bfa6-dec19cd1fedc    0  0.407325    -9.310   0.304721   \n",
              "t_0f90acc7-d5c5-4e53-901d-55610fbd090c    0  0.251502   -12.159   0.702948   \n",
              "t_36b9ad02-095a-443d-a697-6c7285d9410a    0  0.218370   -13.813   0.888889   \n",
              "\n",
              "                                        organism  speechiness       tempo  \\\n",
              "track_id                                                                    \n",
              "t_2e8f4b71-8a0b-4b9c-b7d8-fb5208e87f9f  0.759699     0.033940  100.370003   \n",
              "t_dae2ec0e-ec7b-4b3e-b60c-4a884d0eccb0  0.747436     0.049856  141.334000   \n",
              "t_cf0164dd-1531-4399-bfa6-dec19cd1fedc  0.493154     0.071753  138.889999   \n",
              "t_0f90acc7-d5c5-4e53-901d-55610fbd090c  0.212197     0.029425  133.139008   \n",
              "t_36b9ad02-095a-443d-a697-6c7285d9410a  0.193438     0.032178  152.212006   \n",
              "\n",
              "                                        time_signature   valence  \\\n",
              "track_id                                                           \n",
              "t_2e8f4b71-8a0b-4b9c-b7d8-fb5208e87f9f               4  0.223395   \n",
              "t_dae2ec0e-ec7b-4b3e-b60c-4a884d0eccb0               4  0.484702   \n",
              "t_cf0164dd-1531-4399-bfa6-dec19cd1fedc               4  0.818441   \n",
              "t_0f90acc7-d5c5-4e53-901d-55610fbd090c               4  0.594829   \n",
              "t_36b9ad02-095a-443d-a697-6c7285d9410a               4  0.591289   \n",
              "\n",
              "                                        acoustic_vector_0  acoustic_vector_1  \\\n",
              "track_id                                                                       \n",
              "t_2e8f4b71-8a0b-4b9c-b7d8-fb5208e87f9f           0.146012          -0.706908   \n",
              "t_dae2ec0e-ec7b-4b3e-b60c-4a884d0eccb0           0.039554          -0.539554   \n",
              "t_cf0164dd-1531-4399-bfa6-dec19cd1fedc           0.083863          -0.242108   \n",
              "t_0f90acc7-d5c5-4e53-901d-55610fbd090c           0.192498           0.340039   \n",
              "t_36b9ad02-095a-443d-a697-6c7285d9410a           0.270586          -0.411061   \n",
              "\n",
              "                                        acoustic_vector_2  acoustic_vector_3  \\\n",
              "track_id                                                                       \n",
              "t_2e8f4b71-8a0b-4b9c-b7d8-fb5208e87f9f           0.259496           0.481157   \n",
              "t_dae2ec0e-ec7b-4b3e-b60c-4a884d0eccb0           0.105141           0.692589   \n",
              "t_cf0164dd-1531-4399-bfa6-dec19cd1fedc          -0.014258           0.096396   \n",
              "t_0f90acc7-d5c5-4e53-901d-55610fbd090c           0.034846          -0.389794   \n",
              "t_36b9ad02-095a-443d-a697-6c7285d9410a           0.165898           0.225652   \n",
              "\n",
              "                                        acoustic_vector_4  acoustic_vector_5  \\\n",
              "track_id                                                                       \n",
              "t_2e8f4b71-8a0b-4b9c-b7d8-fb5208e87f9f           0.238427          -0.098389   \n",
              "t_dae2ec0e-ec7b-4b3e-b60c-4a884d0eccb0           0.226047          -0.468162   \n",
              "t_cf0164dd-1531-4399-bfa6-dec19cd1fedc           0.417641          -0.050576   \n",
              "t_0f90acc7-d5c5-4e53-901d-55610fbd090c           0.518381           0.185008   \n",
              "t_36b9ad02-095a-443d-a697-6c7285d9410a           0.335518          -0.036643   \n",
              "\n",
              "                                        acoustic_vector_6  acoustic_vector_7  \\\n",
              "track_id                                                                       \n",
              "t_2e8f4b71-8a0b-4b9c-b7d8-fb5208e87f9f          -0.254960          -0.227383   \n",
              "t_dae2ec0e-ec7b-4b3e-b60c-4a884d0eccb0           0.164389          -0.769024   \n",
              "t_cf0164dd-1531-4399-bfa6-dec19cd1fedc          -0.204757          -0.172563   \n",
              "t_0f90acc7-d5c5-4e53-901d-55610fbd090c          -0.079907          -0.016978   \n",
              "t_36b9ad02-095a-443d-a697-6c7285d9410a          -0.016300          -0.446870   \n",
              "\n",
              "                                        mode_major  mode_minor  duration_60  \\\n",
              "track_id                                                                      \n",
              "t_2e8f4b71-8a0b-4b9c-b7d8-fb5208e87f9f           1           0            0   \n",
              "t_dae2ec0e-ec7b-4b3e-b60c-4a884d0eccb0           1           0            0   \n",
              "t_cf0164dd-1531-4399-bfa6-dec19cd1fedc           1           0            0   \n",
              "t_0f90acc7-d5c5-4e53-901d-55610fbd090c           1           0            0   \n",
              "t_36b9ad02-095a-443d-a697-6c7285d9410a           1           0            0   \n",
              "\n",
              "                                        duration_90  duration_120  \\\n",
              "track_id                                                            \n",
              "t_2e8f4b71-8a0b-4b9c-b7d8-fb5208e87f9f            0             0   \n",
              "t_dae2ec0e-ec7b-4b3e-b60c-4a884d0eccb0            0             0   \n",
              "t_cf0164dd-1531-4399-bfa6-dec19cd1fedc            0             1   \n",
              "t_0f90acc7-d5c5-4e53-901d-55610fbd090c            0             0   \n",
              "t_36b9ad02-095a-443d-a697-6c7285d9410a            0             0   \n",
              "\n",
              "                                        duration_150  duration_180  \\\n",
              "track_id                                                             \n",
              "t_2e8f4b71-8a0b-4b9c-b7d8-fb5208e87f9f             0             0   \n",
              "t_dae2ec0e-ec7b-4b3e-b60c-4a884d0eccb0             1             0   \n",
              "t_cf0164dd-1531-4399-bfa6-dec19cd1fedc             0             0   \n",
              "t_0f90acc7-d5c5-4e53-901d-55610fbd090c             0             0   \n",
              "t_36b9ad02-095a-443d-a697-6c7285d9410a             0             1   \n",
              "\n",
              "                                        duration_210  duration_240  \\\n",
              "track_id                                                             \n",
              "t_2e8f4b71-8a0b-4b9c-b7d8-fb5208e87f9f             0             0   \n",
              "t_dae2ec0e-ec7b-4b3e-b60c-4a884d0eccb0             0             0   \n",
              "t_cf0164dd-1531-4399-bfa6-dec19cd1fedc             0             0   \n",
              "t_0f90acc7-d5c5-4e53-901d-55610fbd090c             0             1   \n",
              "t_36b9ad02-095a-443d-a697-6c7285d9410a             0             0   \n",
              "\n",
              "                                        duration_270  duration_300  \\\n",
              "track_id                                                             \n",
              "t_2e8f4b71-8a0b-4b9c-b7d8-fb5208e87f9f             0             0   \n",
              "t_dae2ec0e-ec7b-4b3e-b60c-4a884d0eccb0             0             0   \n",
              "t_cf0164dd-1531-4399-bfa6-dec19cd1fedc             0             0   \n",
              "t_0f90acc7-d5c5-4e53-901d-55610fbd090c             0             0   \n",
              "t_36b9ad02-095a-443d-a697-6c7285d9410a             0             0   \n",
              "\n",
              "                                        duration_long  duration_very_long  \\\n",
              "track_id                                                                    \n",
              "t_2e8f4b71-8a0b-4b9c-b7d8-fb5208e87f9f              1                   0   \n",
              "t_dae2ec0e-ec7b-4b3e-b60c-4a884d0eccb0              0                   0   \n",
              "t_cf0164dd-1531-4399-bfa6-dec19cd1fedc              0                   0   \n",
              "t_0f90acc7-d5c5-4e53-901d-55610fbd090c              0                   0   \n",
              "t_36b9ad02-095a-443d-a697-6c7285d9410a              0                   0   \n",
              "\n",
              "                                        release_year_60s  release_year_70s  \\\n",
              "track_id                                                                     \n",
              "t_2e8f4b71-8a0b-4b9c-b7d8-fb5208e87f9f                 0                 0   \n",
              "t_dae2ec0e-ec7b-4b3e-b60c-4a884d0eccb0                 0                 1   \n",
              "t_cf0164dd-1531-4399-bfa6-dec19cd1fedc                 0                 0   \n",
              "t_0f90acc7-d5c5-4e53-901d-55610fbd090c                 0                 0   \n",
              "t_36b9ad02-095a-443d-a697-6c7285d9410a                 0                 0   \n",
              "\n",
              "                                        release_year_80s  release_year_90s  \\\n",
              "track_id                                                                     \n",
              "t_2e8f4b71-8a0b-4b9c-b7d8-fb5208e87f9f                 1                 0   \n",
              "t_dae2ec0e-ec7b-4b3e-b60c-4a884d0eccb0                 0                 0   \n",
              "t_cf0164dd-1531-4399-bfa6-dec19cd1fedc                 1                 0   \n",
              "t_0f90acc7-d5c5-4e53-901d-55610fbd090c                 0                 1   \n",
              "t_36b9ad02-095a-443d-a697-6c7285d9410a                 0                 1   \n",
              "\n",
              "                                        release_year_00s  release_year_10s  \\\n",
              "track_id                                                                     \n",
              "t_2e8f4b71-8a0b-4b9c-b7d8-fb5208e87f9f                 0                 0   \n",
              "t_dae2ec0e-ec7b-4b3e-b60c-4a884d0eccb0                 0                 0   \n",
              "t_cf0164dd-1531-4399-bfa6-dec19cd1fedc                 0                 0   \n",
              "t_0f90acc7-d5c5-4e53-901d-55610fbd090c                 0                 0   \n",
              "t_36b9ad02-095a-443d-a697-6c7285d9410a                 0                 0   \n",
              "\n",
              "                                        release_year_20s  \n",
              "track_id                                                  \n",
              "t_2e8f4b71-8a0b-4b9c-b7d8-fb5208e87f9f                 0  \n",
              "t_dae2ec0e-ec7b-4b3e-b60c-4a884d0eccb0                 0  \n",
              "t_cf0164dd-1531-4399-bfa6-dec19cd1fedc                 0  \n",
              "t_0f90acc7-d5c5-4e53-901d-55610fbd090c                 0  \n",
              "t_36b9ad02-095a-443d-a697-6c7285d9410a                 0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRXn-mhJvdC4",
        "outputId": "ff1755ec-cea7-4454-a266-b2770517f1d1"
      },
      "source": [
        "track_features.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['us_popularity_estimate', 'acousticness', 'beat_strength', 'bounciness',\n",
              "       'danceability', 'dyn_range_mean', 'energy', 'flatness',\n",
              "       'instrumentalness', 'key', 'liveness', 'loudness', 'mechanism',\n",
              "       'organism', 'speechiness', 'tempo', 'time_signature', 'valence',\n",
              "       'acoustic_vector_0', 'acoustic_vector_1', 'acoustic_vector_2',\n",
              "       'acoustic_vector_3', 'acoustic_vector_4', 'acoustic_vector_5',\n",
              "       'acoustic_vector_6', 'acoustic_vector_7', 'mode_major', 'mode_minor',\n",
              "       'duration_60', 'duration_90', 'duration_120', 'duration_150',\n",
              "       'duration_180', 'duration_210', 'duration_240', 'duration_270',\n",
              "       'duration_300', 'duration_long', 'duration_very_long',\n",
              "       'release_year_60s', 'release_year_70s', 'release_year_80s',\n",
              "       'release_year_90s', 'release_year_00s', 'release_year_10s',\n",
              "       'release_year_20s'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOhvw0AZrLMy"
      },
      "source": [
        "## Logs Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GU7f4b92snU2"
      },
      "source": [
        "def day_binning(logs):\n",
        "  ''' Binning function.\n",
        "  '''\n",
        "  cut_bins = [0, 12, 18, 24]\n",
        "  bin_names = ['morning', 'afternoon', 'evening']\n",
        "  logs['hour_of_day'] = pd.cut(logs['hour_of_day'], bins=cut_bins, labels=bin_names)\n",
        "  logs = categorical_to_dummies(logs, ['hour_of_day'])\n",
        "  return logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coo3mpXetPWm"
      },
      "source": [
        "def cosine_skipped(data):\n",
        "  ''' Calculate cosine distance between mean of second part of session and each track.\n",
        "  '''\n",
        "  cosine_distance_columns = ['duration_60', 'duration_90', 'duration_120', 'duration_150',\n",
        "       'duration_180', 'duration_210', 'duration_240', 'duration_270',\n",
        "       'duration_300', 'duration_long', 'duration_very_long', 'release_year_60s', 'release_year_70s', 'release_year_80s',\n",
        "       'release_year_90s', 'release_year_00s', 'release_year_10s',\n",
        "       'release_year_20s', 'us_popularity_estimate', 'acousticness',\n",
        "       'beat_strength', 'bounciness', 'danceability', 'dyn_range_mean',\n",
        "       'energy', 'flatness', 'instrumentalness', 'key', 'liveness', 'loudness',\n",
        "       'mechanism', 'organism', 'speechiness', 'tempo', 'time_signature',\n",
        "       'valence', 'acoustic_vector_0', 'acoustic_vector_1',\n",
        "       'acoustic_vector_2', 'acoustic_vector_3', 'acoustic_vector_4',\n",
        "       'acoustic_vector_5', 'acoustic_vector_6', 'acoustic_vector_7']\n",
        "  second_half_songs_data = data[data['session_position'] > 0.5 * data['session_length']]\n",
        "  mean_second_half_songs = second_half_songs_data[cosine_distance_columns].mean().tolist()\n",
        "  data['similarity_mean_second_half_songs'] = data[cosine_distance_columns].apply(lambda x: cosine_similarity([mean_second_half_songs], [x.tolist()])[0][0], axis=1)\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsiydPF8rkZW"
      },
      "source": [
        "def logs_cleaning(data):\n",
        "  ''' Cleans data. \n",
        "  args:\n",
        "    data: dataframe to clean.\n",
        "  returns:\n",
        "    data: cleaned dataframe.\n",
        "  '''\n",
        "  # remove date for convenience (could encode this as well)\n",
        "  data.drop(columns=['date'], inplace=True)\n",
        "  # Create dummies (one hot encoding) for each categorical variable in logs\n",
        "  categorical_cols = ['context_type', 'hist_user_behavior_reason_start', 'hist_user_behavior_reason_end']\n",
        "  data = categorical_to_dummies(data, categorical_cols)\n",
        "\n",
        "  # Convert booleans to ints\n",
        "  data['premium'] = data['premium']*1\n",
        "  data['hist_user_behavior_is_shuffle'] = data['hist_user_behavior_is_shuffle']*1\n",
        "  data['skip_1'] = data['skip_1']*1\n",
        "  data['skip_2'] = data['skip_2']*1\n",
        "  data['skip_3'] = data['skip_3']*1\n",
        "\n",
        "  # Normalize\n",
        "  feature_name = [\n",
        "  'us_popularity_estimate',\n",
        "  'flatness',\n",
        "  'loudness',\n",
        "  'tempo',\n",
        "  'acoustic_vector_0',\n",
        "  'acoustic_vector_1',\n",
        "  'acoustic_vector_2',\n",
        "  'acoustic_vector_3',\n",
        "  'acoustic_vector_4',\n",
        "  'acoustic_vector_5',\n",
        "  'acoustic_vector_6',\n",
        "  'acoustic_vector_7',\n",
        "  'key']\n",
        "\n",
        "  data = normalize(data, feature_name)\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ydR4vq5rjKn"
      },
      "source": [
        "def logs_feature_joining(data):\n",
        "  ''' Joins a chunk of data from the session logs with the track features. \n",
        "  args:\n",
        "    data: dataframe to join features to.\n",
        "  returns:\n",
        "    data: dataframe including logs and track feature columns.\n",
        "  '''\n",
        "  data = data.join(track_features, on='track_id_clean', how='left')\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkCtby9BrNz-"
      },
      "source": [
        "def data_generator(files, chunksize=10000):\n",
        "  ''' Will infinitely generate chunks of data from all csv files.\n",
        "  args:\n",
        "    files: list of path names to CSV files holding session logs.\n",
        "    chunksize: CSV files will be read in chunks of size chunksize.\n",
        "\n",
        "  Note:\n",
        "    Chunk size isn't uniform, will only return full sessions.\n",
        "    So, one chunk might be of length 997, next might be 1005.\n",
        "    Reason being that chunks can split a session in two, this is unwanted behavior.\n",
        "  '''\n",
        "  assert isinstance(files, list), \"files argument should be list of paths\"\n",
        "  while True:\n",
        "    iterator_generator = (pd.read_csv(f, iterator=True, chunksize=chunksize) for f in files)\n",
        "    dummy = pd.DataFrame()\n",
        "    for iterator in iterator_generator:\n",
        "      print('\\n### Opened new file')\n",
        "      for chunk in iterator:\n",
        "        # Get position of last row element\n",
        "        last_position = chunk.iloc[-1]['session_position']\n",
        "        # Slice last session\n",
        "        last_session = chunk.iloc[-last_position:]\n",
        "        # Drop last session from current chunk\n",
        "        chunk.drop(last_session.index, inplace=True)\n",
        "        # Append chunk to previous last session (to get a full session)\n",
        "        dummy = dummy.append(chunk)\n",
        "        yield dummy\n",
        "        # Assign this last session to dummy for next session\n",
        "        dummy = last_session\n",
        "  print('\\n### Processed all Files')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xS1XeM9NrVe1"
      },
      "source": [
        "def data_processor(files, filename, chunksize=20000, delete=False):\n",
        "  ''' Pipeline: will clean, join,... each chunk and save to csv.\n",
        "  args:\n",
        "    files (list): list of path names to CSV files holding session logs.\n",
        "    chunksize (int): CSV files will be read in chunks of size chunksize.\n",
        "    filename (string): path to save file to.\n",
        "    delete (bool): delete file at filename first or not.\n",
        "  '''\n",
        "  generator = data_generator(files, chunksize)\n",
        "  header = True\n",
        "  if delete:\n",
        "    os.remove(filename)\n",
        "  for chunk in generator:\n",
        "    chunk = logs_feature_joining(chunk)\n",
        "    chunk = logs_cleaning(chunk)\n",
        "    chunk = day_binning(chunk)\n",
        "    chunk = cosine_skipped(chunk)\n",
        "    chunk.to_csv(filename, header=True, mode='a')\n",
        "    header = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "Da3fJOiLvwJy",
        "outputId": "2e8d7088-f99d-4b1b-8d90-f6c22516bb45"
      },
      "source": [
        "# For files 1-7:\n",
        "data_processor([files[0]], filename='/content/drive/MyDrive/CS/AI/Data/large_logs_processed.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-4ee53200a7a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# For files 1-7:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata_processor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/MyDrive/CS/AI/Data/large_logs_processed.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-8c6633ef5e2b>\u001b[0m in \u001b[0;36mdata_processor\u001b[0;34m(files, filename, chunksize, delete)\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdelete\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs_feature_joining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs_cleaning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-f63fc160af3c>\u001b[0m in \u001b[0;36mdata_generator\u001b[0;34m(files, chunksize)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0miterator_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mdummy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterator_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n### Opened new file'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-f63fc160af3c>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"files argument should be list of paths\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0miterator_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mdummy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterator_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/_compression.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"B\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbyte_view\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_view\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mbyte_view\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/gzip.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    461\u001b[0m                 \u001b[0;31m# jump to the next member, if there is one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_gzip_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/gzip.py\u001b[0m in \u001b[0;36m_read_gzip_header\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_gzip_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m         \u001b[0mmagic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmagic\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/gzip.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                    \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_length\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprepend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fsspec/spec.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, length)\u001b[0m\n\u001b[1;32m   1397\u001b[0m             \u001b[0;31m# don't even bother calling fetch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1398\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fsspec/caching.py\u001b[0m in \u001b[0;36m_fetch\u001b[0;34m(self, start, end)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mpart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# new block replaces old\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gcsfs/core.py\u001b[0m in \u001b[0;36m_fetch_range\u001b[0;34m(self, start, end)\u001b[0m\n\u001b[1;32m   1455\u001b[0m             \u001b[0mhead\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1457\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcsfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GET\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetails\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mediaLink\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1458\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fsspec/asyn.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_sync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fsspec/asyn.py\u001b[0m in \u001b[0;36mmaybe_sync\u001b[0;34m(func, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miscoroutinefunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;31m# run the awaitable on the loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0;31m# just call the blocking function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fsspec/asyn.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(loop, func, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIIPqOk6gw0L"
      },
      "source": [
        "# For file 8:\n",
        "data_processor([files[0]], filename='/content/drive/MyDrive/CS/AI/Data/large_logs_processed.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jJ1USuR0Bl9"
      },
      "source": [
        "## Training & Testing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-c7fQEB9vmY"
      },
      "source": [
        "# What proably needs to be done:\n",
        "# 1) create two pre-processed files: train and test\n",
        "# 2) performs spitting and stacking as usual -> maybe even do this as another pre-processing step\n",
        "# 3) Do imputing and stuff\n",
        "# 4) train the tensorflow GBT model whatever way tf does it (dataset API probably)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQ82b6kfckd0"
      },
      "source": [
        "def stack_sessions(df):\n",
        "    \"\"\"\n",
        "    Turn matrix representation into vector by stacking the listen events together (as columns) \n",
        "    For example:\n",
        "    session_id session_position feature1 feature2\n",
        "    a          1                ~        ~\n",
        "    a          2                ~        ~\n",
        "    b          1                ~        ~\n",
        "    b          2                ~        ~\n",
        "    b          3                ~        ~\n",
        "    \n",
        "    Turns into:\n",
        "    session_id 1_feature1 1_feature2 2_feature1 2_feature2 3_feature1 3_feature2\n",
        "    a          ~          ~          ~          ~          Nan        Nan\n",
        "    b          ~          ~          ~          ~          ~          ~\n",
        "    \"\"\"\n",
        "    columns = list(df.columns)\n",
        "    columns.remove('session_id')\n",
        "    columns.remove('session_position')\n",
        "    sessions = df.pivot(index='session_id', columns='session_position', values=columns)\n",
        "    return sessions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGr0wcCac1Fz"
      },
      "source": [
        "def split_df(df):\n",
        "    \"\"\"\n",
        "    Split df in data and labels part. First half of session is stacked and joined to each song in the second half of the session.\n",
        "    Listening information is removed from second half, as it will not be available for prediction.\n",
        "    \"\"\"\n",
        "    drop_cols = ['track_id_clean', 'Unnamed: 0']\n",
        "    df.drop(columns=drop_cols, inplace=True)\n",
        "\n",
        "    first = df.loc[df['session_position']*2 <= df['session_length']].reset_index().drop(columns=['session_length'])\n",
        "    second = df.loc[df['session_position']*2 > df['session_length']].reset_index()\n",
        "    truth = second['skip_2']\n",
        "\n",
        "    # After resetting index, need to remove index from chunk\n",
        "    first.drop(columns=['index'], inplace=True)\n",
        "    second.drop(columns=['index'], inplace=True)\n",
        "\n",
        "    drop_cols = list(second.columns)\n",
        "    drop_cols.remove('session_id')\n",
        "    drop_cols.remove('session_position')\n",
        "    # Need to specify which columns you have access to in the second half:\n",
        "    # print(second.columns)\n",
        "    columns_second = ['us_popularity_estimate',\n",
        "       'acousticness', 'beat_strength', 'bounciness', 'danceability',\n",
        "       'dyn_range_mean', 'energy', 'flatness', 'instrumentalness', 'key',\n",
        "       'liveness', 'loudness', 'mechanism', 'organism', 'speechiness', 'tempo',\n",
        "       'time_signature', 'valence', 'acoustic_vector_0', 'acoustic_vector_1',\n",
        "       'acoustic_vector_2', 'acoustic_vector_3', 'acoustic_vector_4',\n",
        "       'acoustic_vector_5', 'acoustic_vector_6', 'acoustic_vector_7',\n",
        "       'mode_major', 'mode_minor', 'duration_60', 'duration_90',\n",
        "       'duration_120', 'duration_150', 'duration_180', 'duration_210',\n",
        "       'duration_240', 'duration_270', 'duration_300', 'duration_long',\n",
        "       'duration_very_long', 'release_year_60s', 'release_year_70s',\n",
        "       'release_year_80s', 'release_year_90s', 'release_year_00s',\n",
        "       'release_year_10s', 'release_year_20s', 'similarity_mean_second_half_songs']\n",
        "    for elem in columns_second:\n",
        "      if elem in drop_cols:\n",
        "        drop_cols.remove(elem)\n",
        "    second.drop(columns=drop_cols, inplace=True)\n",
        "    first_stacked = stack_sessions(first)\n",
        "\n",
        "    data = second.join(first_stacked, how='left', on='session_id')\n",
        "    data.drop(columns=['session_id'], inplace=True)\n",
        "    return data, truth"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGEHdCfGc6Fo"
      },
      "source": [
        "def dataset_generator(chunksize=10000):\n",
        "  file = '/content/drive/MyDrive/CS/AI/Data/large_logs_processed.csv'\n",
        "  generator = data_generator([file], chunksize=10000)\n",
        "  for chunk in generator:\n",
        "    chunk, truth = split_df(chunk)\n",
        "    # Need to impute too!\n",
        "    # NEED TO CREATE THE WEIRD FORMAT HERE\n",
        "    features = chunk.to_dict(orient='list')\n",
        "    labels = truth\n",
        "    # print(type(truth))\n",
        "    yield features, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaADgg_4fzdT",
        "outputId": "bff576d6-94aa-4746-8b58-504d84180c5b"
      },
      "source": [
        "# Testing\n",
        "dataset = tf.data.Dataset.from_generator(dataset_generator, (tf.float32, tf.float32))\n",
        "list(dataset.take(1).as_numpy_iterator())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "### Opened new file\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/reshape/merge.py:643: UserWarning: merging between different levels can give an unintended result (1 levels on the left,2 on the right)\n",
            "  warnings.warn(msg, UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kvpo9toh8Nst"
      },
      "source": [
        "# Problem is in this function, don't understand how input_fn works exactly\n",
        "def make_input_fn(n_epochs=None, shuffle=False):\n",
        "  def input_fn():\n",
        "    # INPUT_FN should return: \n",
        "    # A tf.data.Dataset object: Outputs of Dataset object must be a tuple (features, labels) with same constraints as below.\n",
        "    # A tuple (features, labels): Where features is a tf.Tensor or a dictionary of string feature name to Tensor and labels is a Tensor or a dictionary of string label name to Tensor. Both features and labels are consumed by model_fn. They should satisfy the expectation of model_fn from inputs. \n",
        "    dataset = tf.data.Dataset.from_generator(dataset_generator, (tf.float64, tf.int32), (tf.TensorShape([878,]), tf.TensorShape([])))\n",
        "    # For training, cycle thru dataset as many times as need (n_epochs=None).\n",
        "    dataset = dataset.repeat(n_epochs)\n",
        "    return dataset\n",
        "  return input_fn\n",
        "\n",
        "# Training and evaluation input functions.\n",
        "train_input_fn = make_input_fn()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "YZ0NXGqtgm4j",
        "outputId": "5728c895-c225-4fbd-e855-44a8afadc48e"
      },
      "source": [
        "# Need to determine all feature columns that will be used:\n",
        "# Loop through all column names, and create fc.numeric_column objects -> append to list.\n",
        "# We ony have numeric columns.\n",
        "def get_columns(chunksize=1000):\n",
        "  file = '/content/drive/MyDrive/CS/AI/Data/large_logs_processed.csv'\n",
        "  generator = data_generator([file], chunksize=1000)\n",
        "  for chunk in generator:\n",
        "    chunk, truth = split_df(chunk)\n",
        "    for index, row in chunk.iterrows():\n",
        "      return row.index.tolist()\n",
        "columns = get_columns()\n",
        "\n",
        "fc = tf.feature_column\n",
        "feature_columns = []\n",
        "\n",
        "for feature_name in columns:\n",
        "  feature_columns.append(fc.numeric_column(str(feature_name), dtype=tf.float64))\n",
        "\n",
        "# Futher explanation about this function:\n",
        "# Tensorflow BoostedTreesClassifier is very picky in what data it will use.\n",
        "# 1) You need to specify for each column in the input what datatype it is. (see the feature_columns parameter)\n",
        "# 2) When you use generators to create a tf dataset, each column needs to be the same datatype."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "### Opened new file\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-cb093a488ff2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mfc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-cb093a488ff2>\u001b[0m in \u001b[0;36mget_columns\u001b[0;34m(chunksize)\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'split_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1Q3ajL00PMp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        },
        "outputId": "b5e6a885-e081-4828-96b7-fcf0fde308cb"
      },
      "source": [
        "params = {\n",
        "  'n_trees': 100,\n",
        "  'max_depth': 5,\n",
        "  'n_batches_per_layer': 1,\n",
        "  'center_bias': True\n",
        "}\n",
        "classifier = tf.estimator.BoostedTreesClassifier(feature_columns, **params)\n",
        "classifier.train(train_input_fn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using default config.\n",
            "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpxcbj4qey\n",
            "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpxcbj4qey', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-283-68bfcc506bc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m }\n\u001b[1;32m      7\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBoostedTreesClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1173\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1202\u001b[0m       \u001b[0mworker_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m       estimator_spec = self._call_model_fn(features, labels, ModeKeys.TRAIN,\n\u001b[0;32m-> 1204\u001b[0;31m                                            self.config)\n\u001b[0m\u001b[1;32m   1205\u001b[0m       \u001b[0mglobal_step_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, mode, config)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1163\u001b[0;31m     \u001b[0mmodel_fn_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1164\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/canned/boosted_trees.py\u001b[0m in \u001b[0;36m_model_fn\u001b[0;34m(features, labels, mode, config)\u001b[0m\n\u001b[1;32m   2059\u001b[0m           \u001b[0mclosed_form_grad_and_hess_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclosed_form\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2060\u001b[0m           \u001b[0mweight_column\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_column\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2061\u001b[0;31m           train_in_memory=train_in_memory)\n\u001b[0m\u001b[1;32m   2062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2063\u001b[0m     super(BoostedTreesClassifier, self).__init__(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/canned/boosted_trees.py\u001b[0m in \u001b[0;36m_bt_model_fn\u001b[0;34m(features, labels, mode, head, feature_columns, tree_hparams, n_batches_per_layer, config, closed_form_grad_and_hess_fn, example_id_column_name, weight_column, train_in_memory, name)\u001b[0m\n\u001b[1;32m   1247\u001b[0m       input_feature_list = _get_transformed_features(features,\n\u001b[1;32m   1248\u001b[0m                                                      \u001b[0msorted_feature_columns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1249\u001b[0;31m                                                      bucket_boundaries_dict)\n\u001b[0m\u001b[1;32m   1250\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mexample_id_column_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m         \u001b[0mexample_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexample_id_column_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/canned/boosted_trees.py\u001b[0m in \u001b[0;36m_get_transformed_features\u001b[0;34m(features, sorted_feature_columns, bucket_boundaries_dict)\u001b[0m\n\u001b[1;32m    168\u001b[0m   return _get_transformed_features_and_merge_with_previously_transformed(\n\u001b[1;32m    169\u001b[0m       \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_feature_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_feature_columns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m       bucket_boundaries_dict)\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/canned/boosted_trees.py\u001b[0m in \u001b[0;36m_get_transformed_features_and_merge_with_previously_transformed\u001b[0;34m(features, sorted_feature_columns, all_sorted_columns, bucket_boundaries_dict, already_transformed_features)\u001b[0m\n\u001b[1;32m    200\u001b[0m   \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m   transformed_features = _apply_feature_transformations(features,\n\u001b[0;32m--> 202\u001b[0;31m                                                         sorted_feature_columns)\n\u001b[0m\u001b[1;32m    203\u001b[0m   \u001b[0mresult_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/canned/boosted_trees.py\u001b[0m in \u001b[0;36m_apply_feature_transformations\u001b[0;34m(features, feature_columns)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     transformed_columns = feature_column_v2._transform_features_v2(\n\u001b[0;32m--> 142\u001b[0;31m         features, v2_columns, state_manager)\n\u001b[0m\u001b[1;32m    143\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mtransformed_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/feature_column/feature_column_v2.py\u001b[0m in \u001b[0;36m_transform_features_v2\u001b[0;34m(features, feature_columns, state_manager)\u001b[0m\n\u001b[1;32m    416\u001b[0m   \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m   with ops.name_scope(\n\u001b[0;32m--> 418\u001b[0;31m       None, default_name='transform_features', values=features.values()):\n\u001b[0m\u001b[1;32m    419\u001b[0m     \u001b[0mtransformation_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFeatureTransformationCache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeature_columns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'values'"
          ]
        }
      ]
    }
  ]
}